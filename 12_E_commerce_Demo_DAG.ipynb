{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxq991109/MF810-Final-Project/blob/main/12_E_commerce_Demo_DAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63LCTIreE2TE"
      },
      "source": [
        "## Playing with Large Datasets\n",
        "\n",
        "In this notebook we play with data showing eCommerce data from a store that sells products across multiple categories. The data is available through Kaggle and represents two months of behavior data (October and November 2019).\n",
        "\n",
        "This type of individual and transactional type data is becoming significantly more common as logging and data gathering operations become more prevalent. They may also offer unique perspectives into consumer demand or other such economics. \n",
        "\n",
        "The data can be found [here](https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store)\n",
        "\n",
        "### Data\n",
        "\n",
        "You can either setup using the Kaggle package or download the directly from the website. If you choose to use the Kaggle package, you will need to set up your account and download your credentials. Instructions can be found [here](https://github.com/Kaggle/kaggle-api). You basically need to download kaggle.json credential file and save it to your ~/.kaggle/ folder in Docker container. Note: This dataset is 14G spreaded between two files. In this notebook, it loads 2019-Oct.csv file which is 5.3G."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FP9tVMfWE2TO",
        "outputId": "9f8fcda8-9f8b-4f4d-d7a9-4c958e2adfda"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l10arzTgE2TR"
      },
      "outputs": [],
      "source": [
        "# Establish a path for our dataset\n",
        "download_path = \"ecommerce\"\n",
        "\n",
        "# The location for our parquet file\n",
        "parquet_path = \"ecommerce.parquet\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/\"\n",
        "!chmod 600 \"/content/kaggle.json\""
      ],
      "metadata": {
        "id": "5bEe8AarP1MX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPTiaHARE2TS"
      },
      "source": [
        "##### register Kaggle account and grab API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJPEufgxE2TT",
        "outputId": "871c0608-6657-4136-ea93-0e46df206e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "import kaggle\n",
        "kaggle.api.authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rYkIxIRhGes-"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'mkechinov/ecommerce-behavior-data-from-multi-category-store'\n",
        "kaggle.api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKVFSZvrE2TU"
      },
      "source": [
        "### Spark ETL\n",
        "\n",
        "#### Creating/retrieving the SparkSession\n",
        "\n",
        "We establish our SparkSession and set some memory constraints on our cluster. In this case, we are running in local mode so there is only one executor which is the driver. The master option of local\\[n\\] tells Spark to start in local mode with n threads. An asterisk will run with as many logical cores as your machine has available."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVeNIulRQsU-",
        "outputId": "89952b26-647b-4d2a-ca85-8578fab870d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 36 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 39.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=4462c132bd095855784b478d42bc11e62edfef047a60e8bd6720e569b5837eed\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "LX-_GPNuE2TV",
        "outputId": "9e8c8c4a-ba38-4793-f78a-fa366fbabbf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fe11c24f210>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d1811752086f:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>EcommerceAnalysis</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "MAX_MEMORY = \"16g\"\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"EcommerceAnalysis\") \\\n",
        "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
        "    .config(\"spark.ui.port\", \"4050\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qq8s8o3E2TX"
      },
      "source": [
        "#### Read in CSV files to dataframe\n",
        "\n",
        "This reads all of the CSVs in the path that I've provided since this is a lazy operation, this doesn't execute until an action is taken on the dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b5C8258oE2TY"
      },
      "outputs": [],
      "source": [
        "df = spark \\\n",
        "    .read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(f\"{download_path}/2019-Oct.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8GyGX6E2TZ"
      },
      "source": [
        "#### Convert to Parquet\n",
        "\n",
        "View progress in the Spark Web UI at http://127.0.0.1:4040/jobs/\n",
        "This action takes the CSV files being read in and writes them as compressed parquet files. Look at the output folder with the parquet file and notice that it is partitioned. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F3tZw-iFE2Ta",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"compression\", \"gzip\")\\\n",
        "    .parquet(parquet_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AMm5RAYE2Tb"
      },
      "source": [
        "#### Read from Parquet\n",
        "\n",
        "This simply reads the data back in and overwrites the df variable effectively destroying the original DAG from the CSVs and starting\n",
        "them from the parquet files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8PkLa1FLE2Tc"
      },
      "outputs": [],
      "source": [
        "df = spark.read.parquet(parquet_path).persist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfNov = spark \\\n",
        "    .read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(f\"{download_path}/2019-Nov.csv\")"
      ],
      "metadata": {
        "id": "_n-2OtBqJTRW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YQVOWBkE2Td"
      },
      "source": [
        "### Basic Statistics\n",
        "\n",
        "#### Column schema\n",
        "\n",
        "As mentioned before, the Spark DataFrame allows us to impose a column naming and schema upon the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBT5-XO-E2Te",
        "outputId": "2a9bedf5-c682-4f77-d595-082b850c5bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- event_time: string (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- category_id: long (nullable = true)\n",
            " |-- category_code: string (nullable = true)\n",
            " |-- brand: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- user_session: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf6mO4M7E2Tf",
        "outputId": "872a73ca-5002-4d55-c006-c6b64f0d2d6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42448764"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yDObCqnE2Tg",
        "outputId": "9822ee77-5ebc-4ffa-d31c-faaee6fa6feb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(event_time='2019-10-13 06:25:46 UTC', event_type='view', product_id=1002544, category_id=2053013555631882655, category_code='electronics.smartphone', brand='apple', price=460.51, user_id=518958788, user_session='e7e27c5c-1e78-4812-9f55-cdc658bb40fe')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EftJoFTpE2Th",
        "outputId": "136cecbb-4340-4d2e-adbe-ef74eafb85cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+----------+-------------------+--------------------+-----+------+---------+--------------------+\n",
            "|          event_time|event_type|product_id|        category_id|       category_code|brand| price|  user_id|        user_session|\n",
            "+--------------------+----------+----------+-------------------+--------------------+-----+------+---------+--------------------+\n",
            "|2019-10-13 06:25:...|      view|   1002544|2053013555631882655|electronics.smart...|apple|460.51|518958788|e7e27c5c-1e78-481...|\n",
            "+--------------------+----------+----------+-------------------+--------------------+-----+------+---------+--------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmYhufxpE2Ti"
      },
      "source": [
        "The describe command summarizes the columns of the dataframe. This requires a computation to occur on every column and across the entire dataset; therefore, it will actually take some time to run. On my machine, it takes ~4 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRaDESgeE2Tj",
        "outputId": "18f6566f-be2a-4471-b33f-da804c0417cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+----------+--------------------+--------------------+-------------------+--------+-----------------+-------------------+--------------------+\n",
            "|summary|          event_time|event_type|          product_id|         category_id|      category_code|   brand|            price|            user_id|        user_session|\n",
            "+-------+--------------------+----------+--------------------+--------------------+-------------------+--------+-----------------+-------------------+--------------------+\n",
            "|  count|            42448764|  42448764|            42448764|            42448764|           28933155|36335756|         42448764|           42448764|            42448762|\n",
            "|   mean|                null|      null|1.0549932375842676E7|2.057404237884572...|               null|     NaN|290.3236606850655|5.335371475081686E8|                null|\n",
            "| stddev|                null|      null|1.1881906970608277E7|1.843926466140400...|               null|     NaN|358.2691553394025|1.852373817465447E7|                null|\n",
            "|    min|2019-10-01 00:00:...|      cart|             1000978| 2053013552226107603|    accessories.bag|  a-case|              0.0|           33869381|00000042-3e3f-42f...|\n",
            "|    max|2019-10-31 23:59:...|      view|            60500010| 2175419595093967522|stationery.cartrige|   zyxel|          2574.07|          566280860|fffffc65-7ce9-435...|\n",
            "+-------+--------------------+----------+--------------------+--------------------+-------------------+--------+-----------------+-------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkC_iP41E2Tj",
        "outputId": "cf404811-a9eb-49ee-da22-504e66cdcb0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+\n",
            "|summary|            price|\n",
            "+-------+-----------------+\n",
            "|  count|         42448764|\n",
            "|   mean|290.3236606850655|\n",
            "| stddev|358.2691553394025|\n",
            "|    min|              0.0|\n",
            "|    max|          2574.07|\n",
            "+-------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.describe(\"price\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxSJ0TnmE2Tk"
      },
      "source": [
        "### Data Grammar\n",
        "\n",
        "#### Select - select()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQkLcbIzE2Tk",
        "outputId": "c98d799a-86c3-41bf-f00b-dacc5d2ba179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-------+\n",
            "|          event_time|       category_code|  price|\n",
            "+--------------------+--------------------+-------+\n",
            "|2019-10-13 06:25:...|electronics.smart...| 460.51|\n",
            "|2019-10-13 06:25:...|appliances.enviro...| 120.93|\n",
            "|2019-10-13 06:25:...|                null|  45.05|\n",
            "|2019-10-13 06:25:...|computers.periphe...|  12.56|\n",
            "|2019-10-13 06:25:...|  computers.notebook|1801.82|\n",
            "+--------------------+--------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df \\\n",
        "    .select(\"event_time\",\"category_code\",\"price\") \\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ona8_y0mE2Tl"
      },
      "source": [
        "#### Filter - filter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2elgeA9E2Tl",
        "outputId": "c62ac3ff-dd19-4bdf-b14a-b7218b7ac8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----+\n",
            "|          event_time|       category_code|price|\n",
            "+--------------------+--------------------+-----+\n",
            "|2019-10-13 06:25:...|electronics.smart...|88.26|\n",
            "|2019-10-13 06:25:...|electronics.smart...|98.51|\n",
            "|2019-10-13 06:25:...|electronics.smart...|92.14|\n",
            "|2019-10-13 06:25:...|electronics.smart...|65.61|\n",
            "|2019-10-13 06:25:...|electronics.smart...|98.51|\n",
            "+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df \\\n",
        "    .select(\"event_time\",\"category_code\",\"price\") \\\n",
        "    .filter(\"price < 100.00 and category_code == 'electronics.smartphone'\") \\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWkbwHfqE2Tm"
      },
      "source": [
        "#### Mutate - withColumn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj_sD-jME2Tm",
        "outputId": "0c64c57a-9841-4455-e60f-65bcbcedb483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----+---------+\n",
            "|          event_time|       category_code|price|cad_price|\n",
            "+--------------------+--------------------+-----+---------+\n",
            "|2019-10-13 06:25:...|electronics.smart...|88.26| 121.7988|\n",
            "|2019-10-13 06:25:...|electronics.smart...|98.51| 135.9438|\n",
            "|2019-10-13 06:25:...|electronics.smart...|92.14| 127.1532|\n",
            "|2019-10-13 06:25:...|electronics.smart...|65.61|  90.5418|\n",
            "|2019-10-13 06:25:...|electronics.smart...|98.51| 135.9438|\n",
            "+--------------------+--------------------+-----+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import udf\n",
        "usd_cad_udf = udf(lambda usd: usd * 1.38, FloatType())\n",
        "\n",
        "df \\\n",
        "    .select(\"event_time\",\"category_code\",\"price\") \\\n",
        "    .filter(\"price < 100 and category_code == 'electronics.smartphone'\") \\\n",
        "    .withColumn(\"cad_price\", usd_cad_udf(df.price)) \\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkTUvORQE2Tm"
      },
      "source": [
        "#### Arrange - orderBy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IREJTIDE2Tn",
        "outputId": "2759613b-2265-4add-9a09-09f3099677ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+-----+---------+\n",
            "|          event_time|       category_code|price|cad_price|\n",
            "+--------------------+--------------------+-----+---------+\n",
            "|2019-10-01 15:08:...|electronics.smart...|99.87| 137.8206|\n",
            "|2019-10-01 15:07:...|electronics.smart...|99.87| 137.8206|\n",
            "|2019-10-01 15:07:...|electronics.smart...|99.87| 137.8206|\n",
            "|2019-10-01 15:04:...|electronics.smart...|99.87| 137.8206|\n",
            "|2019-10-01 15:08:...|electronics.smart...|99.87| 137.8206|\n",
            "+--------------------+--------------------+-----+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import udf, desc\n",
        "\n",
        "usd_cad_udf = udf(lambda usd: usd * 1.38, FloatType())\n",
        "\n",
        "df \\\n",
        "    .select(\"event_time\",\"category_code\",\"price\") \\\n",
        "    .filter(\"price < 100 and category_code == 'electronics.smartphone'\") \\\n",
        "    .withColumn(\"cad_price\", usd_cad_udf(df.price)) \\\n",
        "    .orderBy(desc(\"cad_price\")) \\\n",
        "    .show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTG1uy77E2Tn"
      },
      "source": [
        "#### Group By and Summarize - groupBy() and agg()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5uW0MLjE2Tn",
        "outputId": "9a69f20a-38e1-4b05-b96a-e4c564404439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------+\n",
            "|       category_code|       avg(price)|\n",
            "+--------------------+-----------------+\n",
            "|electronics.smart...|84.48657257277912|\n",
            "+--------------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import udf, desc\n",
        "\n",
        "usd_cad_udf = udf(lambda usd: usd * 1.38, FloatType())\n",
        "\n",
        "df \\\n",
        "    .select(\"event_time\",\"category_code\",\"price\") \\\n",
        "    .filter(\"price < 100 and category_code == 'electronics.smartphone'\") \\\n",
        "    .withColumn(\"cad_price\", usd_cad_udf(df.price)) \\\n",
        "    .orderBy(desc(\"cad_price\")) \\\n",
        "    .groupBy(\"category_code\") \\\n",
        "    .agg({\"price\": \"mean\"}) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t78KFSGE2To",
        "outputId": "89f8adac-2bc8-43eb-9965-66e0cf14f12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['category_code], ['category_code, 'avg(price#58) AS avg(price)#3320]\n",
            "+- Sort [cad_price#3311 DESC NULLS LAST], true\n",
            "   +- Project [event_time#52, category_code#56, price#58, <lambda>(price#58) AS cad_price#3311]\n",
            "      +- Filter ((price#58 < cast(100 as double)) AND (category_code#56 = electronics.smartphone))\n",
            "         +- Project [event_time#52, category_code#56, price#58]\n",
            "            +- Relation [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] parquet\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "category_code: string, avg(price): double\n",
            "Aggregate [category_code#56], [category_code#56, avg(price#58) AS avg(price)#3320]\n",
            "+- Sort [cad_price#3311 DESC NULLS LAST], true\n",
            "   +- Project [event_time#52, category_code#56, price#58, <lambda>(price#58) AS cad_price#3311]\n",
            "      +- Filter ((price#58 < cast(100 as double)) AND (category_code#56 = electronics.smartphone))\n",
            "         +- Project [event_time#52, category_code#56, price#58]\n",
            "            +- Relation [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] parquet\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [category_code#56], [category_code#56, avg(price#58) AS avg(price)#3320]\n",
            "+- Project [category_code#56, price#58]\n",
            "   +- Sort [cad_price#3311 DESC NULLS LAST], true\n",
            "      +- Project [category_code#56, price#58, pythonUDF0#3414 AS cad_price#3311]\n",
            "         +- BatchEvalPython [<lambda>(price#58)], [pythonUDF0#3414]\n",
            "            +- Project [category_code#56, price#58]\n",
            "               +- Filter ((isnotnull(price#58) AND isnotnull(category_code#56)) AND ((price#58 < 100.0) AND (category_code#56 = electronics.smartphone)))\n",
            "                  +- InMemoryRelation [event_time#52, event_type#53, product_id#54, category_id#55L, category_code#56, brand#57, price#58, user_id#59, user_session#60], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                        +- *(1) ColumnarToRow\n",
            "                           +- FileScan parquet [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/ecommerce.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<event_time:string,event_type:string,product_id:int,category_id:bigint,category_code:string...\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[category_code#56], functions=[avg(price#58)], output=[category_code#56, avg(price)#3320])\n",
            "   +- Exchange hashpartitioning(category_code#56, 200), ENSURE_REQUIREMENTS, [id=#414]\n",
            "      +- HashAggregate(keys=[category_code#56], functions=[partial_avg(price#58)], output=[category_code#56, sum#3462, count#3463L])\n",
            "         +- Project [category_code#56, price#58]\n",
            "            +- Sort [cad_price#3311 DESC NULLS LAST], true, 0\n",
            "               +- Exchange rangepartitioning(cad_price#3311 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#409]\n",
            "                  +- Project [category_code#56, price#58, pythonUDF0#3414 AS cad_price#3311]\n",
            "                     +- BatchEvalPython [<lambda>(price#58)], [pythonUDF0#3414]\n",
            "                        +- Filter (((isnotnull(price#58) AND isnotnull(category_code#56)) AND (price#58 < 100.0)) AND (category_code#56 = electronics.smartphone))\n",
            "                           +- InMemoryTableScan [category_code#56, price#58], [isnotnull(price#58), isnotnull(category_code#56), (price#58 < 100.0), (category_code#56 = electronics.smartphone)]\n",
            "                                 +- InMemoryRelation [event_time#52, event_type#53, product_id#54, category_id#55L, category_code#56, brand#57, price#58, user_id#59, user_session#60], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                                       +- *(1) ColumnarToRow\n",
            "                                          +- FileScan parquet [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/ecommerce.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<event_time:string,event_type:string,product_id:int,category_id:bigint,category_code:string...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df \\\n",
        "    .select(\"event_time\",\"category_code\",\"price\") \\\n",
        "    .filter(\"price < 100 and category_code == 'electronics.smartphone'\") \\\n",
        "    .withColumn(\"cad_price\", usd_cad_udf(df.price)) \\\n",
        "    .orderBy(desc(\"cad_price\")) \\\n",
        "    .groupBy(\"category_code\") \\\n",
        "    .agg({\"price\": \"mean\"}) \\\n",
        "    .explain(extended=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWDtF6sUE2To"
      },
      "source": [
        "## SparkSQL\n",
        "\n",
        "We can register any DataFrame as a table and query from it using SQL by using the registerTempTable command on the DataFrame and passing a table name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w3Y9W0UE2To",
        "outputId": "af7e755b-9acd-465e-9487-5ea8b578e42d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py:140: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  FutureWarning\n"
          ]
        }
      ],
      "source": [
        "df.registerTempTable(\"ecommerce\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XQYPFggE2Tp"
      },
      "source": [
        "* What does our data look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfGZsnzhE2Tp",
        "outputId": "61b90aca-63c8-4350-a2eb-d1ccc660109c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
            "|          event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n",
            "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
            "|2019-10-13 06:25:...|      view|   1002544|2053013555631882655|electronics.smart...|   apple| 460.51|518958788|e7e27c5c-1e78-481...|\n",
            "|2019-10-13 06:25:...|      view|   3700301|2053013565983425517|appliances.enviro...|   vitek| 120.93|557977070|7afc206c-7259-4be...|\n",
            "|2019-10-13 06:25:...|      view|  49100004|2127425375913902544|                null|    null|  45.05|514456508|9d6837a5-40df-49d...|\n",
            "|2019-10-13 06:25:...|      view|   9200409|2053013552913973497|computers.periphe...|defender|  12.56|512530774|df2d048d-c1ae-41b...|\n",
            "|2019-10-13 06:25:...|      view|   1306558|2053013558920217191|  computers.notebook|    acer|1801.82|523366823|0c7f0449-74d5-4b0...|\n",
            "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from ecommerce limit 5\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkPTxvtwE2Tp"
      },
      "source": [
        "* What are some of the brands in this dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xc3YtWZE2Tp",
        "outputId": "056c1e1e-5c1c-49af-b1b5-a9bbae197bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|   brand|\n",
            "+--------+\n",
            "|yokohama|\n",
            "| tuffoni|\n",
            "|   welss|\n",
            "|    tega|\n",
            "| edifier|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select distinct brand from ecommerce limit 5\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTI4B6hwE2Tq"
      },
      "source": [
        "* Out of all of the events, which brand had the highest average price?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuzFgPINE2Tq",
        "outputId": "f5674bdd-48f4-422b-b69e-d4172d23d81c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------------+-----------------+\n",
            "|    brand|        avg_price|        max_price|\n",
            "+---------+-----------------+-----------------+\n",
            "|climadiff|2393.170979020979|2393.170979020979|\n",
            "+---------+-----------------+-----------------+\n",
            "\n",
            "6.663189763999981\n"
          ]
        }
      ],
      "source": [
        "import timeit\n",
        "t0 = timeit.default_timer()\n",
        "spark.sql(\"\"\"\n",
        "    with avg_prices as (\n",
        "        select brand, avg(price) as avg_price \n",
        "        from ecommerce group by brand),\n",
        "        \n",
        "      max_price as (\n",
        "        select max(avg_price) as max_price from avg_prices)\n",
        "    \n",
        "    select \n",
        "      *\n",
        "    from \n",
        "      avg_prices a\n",
        "      inner join max_price m\n",
        "        on a.avg_price = m.max_price\n",
        "\n",
        "\"\"\").show()\n",
        "t1 = timeit.default_timer()\n",
        "print(t1-t0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtlkIRgZE2Tq",
        "outputId": "1d44a342-6db3-4b5a-9097-49b89297cfb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------------+\n",
            "|    brand|        avg_price|\n",
            "+---------+-----------------+\n",
            "|climadiff|2393.170979020979|\n",
            "+---------+-----------------+\n",
            "\n",
            "4.663765392000187\n"
          ]
        }
      ],
      "source": [
        "t0 = timeit.default_timer()\n",
        "spark.sql(\"\"\"\n",
        "    with avg_prices as (\n",
        "        select brand, avg(price) as avg_price \n",
        "        from ecommerce group by brand)\n",
        "    \n",
        "    select\n",
        "      *\n",
        "    from\n",
        "      avg_prices\n",
        "    order by avg_price desc\n",
        "    limit 1\n",
        "\n",
        "\"\"\").show()\n",
        "t1 = timeit.default_timer()\n",
        "print(t1-t0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oFLSWd-E2Tr",
        "outputId": "01efc448-a4f3-4543-b0e6-b71e50b58c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "CTE [avg_prices, max_price]\n",
            ":  :- 'SubqueryAlias avg_prices\n",
            ":  :  +- 'Aggregate ['brand], ['brand, 'avg('price) AS avg_price#4570]\n",
            ":  :     +- 'UnresolvedRelation [ecommerce], [], false\n",
            ":  +- 'SubqueryAlias max_price\n",
            ":     +- 'Project ['max('avg_price) AS max_price#4571]\n",
            ":        +- 'UnresolvedRelation [avg_prices], [], false\n",
            "+- 'Project [*]\n",
            "   +- 'Join Inner, ('a.avg_price = 'm.max_price)\n",
            "      :- 'SubqueryAlias a\n",
            "      :  +- 'UnresolvedRelation [avg_prices], [], false\n",
            "      +- 'SubqueryAlias m\n",
            "         +- 'UnresolvedRelation [max_price], [], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "brand: string, avg_price: double, max_price: double\n",
            "WithCTE\n",
            ":- CTERelationDef 3\n",
            ":  +- SubqueryAlias avg_prices\n",
            ":     +- Aggregate [brand#57], [brand#57, avg(price#58) AS avg_price#4570]\n",
            ":        +- SubqueryAlias ecommerce\n",
            ":           +- View (`ecommerce`, [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60])\n",
            ":              +- Relation [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] parquet\n",
            ":- CTERelationDef 4\n",
            ":  +- SubqueryAlias max_price\n",
            ":     +- Aggregate [max(avg_price#4570) AS max_price#4571]\n",
            ":        +- SubqueryAlias avg_prices\n",
            ":           +- CTERelationRef 3, true, [brand#57, avg_price#4570]\n",
            "+- Project [brand#57, avg_price#4570, max_price#4571]\n",
            "   +- Join Inner, (avg_price#4570 = max_price#4571)\n",
            "      :- SubqueryAlias a\n",
            "      :  +- SubqueryAlias avg_prices\n",
            "      :     +- CTERelationRef 3, true, [brand#57, avg_price#4570]\n",
            "      +- SubqueryAlias m\n",
            "         +- SubqueryAlias max_price\n",
            "            +- CTERelationRef 4, true, [max_price#4571]\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Join Inner, (knownfloatingpointnormalized(normalizenanandzero(avg_price#4570)) = knownfloatingpointnormalized(normalizenanandzero(max_price#4571)))\n",
            ":- Filter isnotnull(avg_price#4570)\n",
            ":  +- Aggregate [brand#57], [brand#57, avg(price#58) AS avg_price#4570]\n",
            ":     +- Project [brand#57, price#58]\n",
            ":        +- InMemoryRelation [event_time#52, event_type#53, product_id#54, category_id#55L, category_code#56, brand#57, price#58, user_id#59, user_session#60], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            ":              +- *(1) ColumnarToRow\n",
            ":                 +- FileScan parquet [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/ecommerce.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<event_time:string,event_type:string,product_id:int,category_id:bigint,category_code:string...\n",
            "+- Filter isnotnull(max_price#4571)\n",
            "   +- Aggregate [max(avg_price#4570) AS max_price#4571]\n",
            "      +- Aggregate [brand#57], [avg(price#58) AS avg_price#4570]\n",
            "         +- Project [brand#57, price#58]\n",
            "            +- InMemoryRelation [event_time#52, event_type#53, product_id#54, category_id#55L, category_code#56, brand#57, price#58, user_id#59, user_session#60], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                  +- *(1) ColumnarToRow\n",
            "                     +- FileScan parquet [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/ecommerce.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<event_time:string,event_type:string,product_id:int,category_id:bigint,category_code:string...\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- BroadcastHashJoin [knownfloatingpointnormalized(normalizenanandzero(avg_price#4570))], [knownfloatingpointnormalized(normalizenanandzero(max_price#4571))], Inner, BuildRight, false\n",
            "   :- Filter isnotnull(avg_price#4570)\n",
            "   :  +- HashAggregate(keys=[brand#57], functions=[avg(price#58)], output=[brand#57, avg_price#4570])\n",
            "   :     +- Exchange hashpartitioning(brand#57, 200), ENSURE_REQUIREMENTS, [id=#882]\n",
            "   :        +- HashAggregate(keys=[brand#57], functions=[partial_avg(price#58)], output=[brand#57, sum#4759, count#4760L])\n",
            "   :           +- InMemoryTableScan [brand#57, price#58]\n",
            "   :                 +- InMemoryRelation [event_time#52, event_type#53, product_id#54, category_id#55L, category_code#56, brand#57, price#58, user_id#59, user_session#60], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "   :                       +- *(1) ColumnarToRow\n",
            "   :                          +- FileScan parquet [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/ecommerce.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<event_time:string,event_type:string,product_id:int,category_id:bigint,category_code:string...\n",
            "   +- BroadcastExchange HashedRelationBroadcastMode(List(knownfloatingpointnormalized(normalizenanandzero(input[0, double, false]))),false), [id=#893]\n",
            "      +- Filter isnotnull(max_price#4571)\n",
            "         +- HashAggregate(keys=[], functions=[max(avg_price#4570)], output=[max_price#4571])\n",
            "            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#889]\n",
            "               +- HashAggregate(keys=[], functions=[partial_max(avg_price#4570)], output=[max#4762])\n",
            "                  +- HashAggregate(keys=[brand#57], functions=[avg(price#58)], output=[avg_price#4570])\n",
            "                     +- Exchange hashpartitioning(brand#57, 200), ENSURE_REQUIREMENTS, [id=#885]\n",
            "                        +- HashAggregate(keys=[brand#57], functions=[partial_avg(price#58)], output=[brand#57, sum#4759, count#4760L])\n",
            "                           +- InMemoryTableScan [brand#57, price#58]\n",
            "                                 +- InMemoryRelation [event_time#52, event_type#53, product_id#54, category_id#55L, category_code#56, brand#57, price#58, user_id#59, user_session#60], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                                       +- *(1) ColumnarToRow\n",
            "                                          +- FileScan parquet [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/ecommerce.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<event_time:string,event_type:string,product_id:int,category_id:bigint,category_code:string...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "    with \n",
        "    \n",
        "      avg_prices as (\n",
        "        select brand, avg(price) as avg_price \n",
        "        from ecommerce group by brand),\n",
        "        \n",
        "      max_price as (\n",
        "        select max(avg_price) as max_price from avg_prices)\n",
        "    \n",
        "    select \n",
        "      *\n",
        "    from \n",
        "      avg_prices a\n",
        "      inner join max_price m\n",
        "        on a.avg_price = m.max_price\n",
        "\n",
        "\"\"\").explain(extended=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zza8YKGOE2Ts",
        "outputId": "d9f24bb8-33a6-4a81-efd0-a3bef538164a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "CTE [avg_prices]\n",
            ":  +- 'SubqueryAlias avg_prices\n",
            ":     +- 'Aggregate ['brand], ['brand, 'avg('price) AS avg_price#4763]\n",
            ":        +- 'UnresolvedRelation [ecommerce], [], false\n",
            "+- 'GlobalLimit 1\n",
            "   +- 'LocalLimit 1\n",
            "      +- 'Sort ['avg_price DESC NULLS LAST], true\n",
            "         +- 'Project [*]\n",
            "            +- 'UnresolvedRelation [avg_prices], [], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "brand: string, avg_price: double\n",
            "WithCTE\n",
            ":- CTERelationDef 5\n",
            ":  +- SubqueryAlias avg_prices\n",
            ":     +- Aggregate [brand#57], [brand#57, avg(price#58) AS avg_price#4763]\n",
            ":        +- SubqueryAlias ecommerce\n",
            ":           +- View (`ecommerce`, [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60])\n",
            ":              +- Relation [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] parquet\n",
            "+- GlobalLimit 1\n",
            "   +- LocalLimit 1\n",
            "      +- Sort [avg_price#4763 DESC NULLS LAST], true\n",
            "         +- Project [brand#57, avg_price#4763]\n",
            "            +- SubqueryAlias avg_prices\n",
            "               +- CTERelationRef 5, true, [brand#57, avg_price#4763]\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "GlobalLimit 1\n",
            "+- LocalLimit 1\n",
            "   +- Sort [avg_price#4763 DESC NULLS LAST], true\n",
            "      +- Aggregate [brand#57], [brand#57, avg(price#58) AS avg_price#4763]\n",
            "         +- Project [brand#57, price#58]\n",
            "            +- InMemoryRelation [event_time#52, event_type#53, product_id#54, category_id#55L, category_code#56, brand#57, price#58, user_id#59, user_session#60], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                  +- *(1) ColumnarToRow\n",
            "                     +- FileScan parquet [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/ecommerce.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<event_time:string,event_type:string,product_id:int,category_id:bigint,category_code:string...\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- TakeOrderedAndProject(limit=1, orderBy=[avg_price#4763 DESC NULLS LAST], output=[brand#57,avg_price#4763])\n",
            "   +- HashAggregate(keys=[brand#57], functions=[avg(price#58)], output=[brand#57, avg_price#4763])\n",
            "      +- Exchange hashpartitioning(brand#57, 200), ENSURE_REQUIREMENTS, [id=#909]\n",
            "         +- HashAggregate(keys=[brand#57], functions=[partial_avg(price#58)], output=[brand#57, sum#4904, count#4905L])\n",
            "            +- InMemoryTableScan [brand#57, price#58]\n",
            "                  +- InMemoryRelation [event_time#52, event_type#53, product_id#54, category_id#55L, category_code#56, brand#57, price#58, user_id#59, user_session#60], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                        +- *(1) ColumnarToRow\n",
            "                           +- FileScan parquet [event_time#52,event_type#53,product_id#54,category_id#55L,category_code#56,brand#57,price#58,user_id#59,user_session#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/ecommerce.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<event_time:string,event_type:string,product_id:int,category_id:bigint,category_code:string...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "    with avg_prices as (\n",
        "        select brand, avg(price) as avg_price \n",
        "        from ecommerce group by brand)\n",
        "    \n",
        "    select\n",
        "      *\n",
        "    from\n",
        "      avg_prices\n",
        "    order by avg_price desc\n",
        "    limit 1\n",
        "\n",
        "\"\"\").explain(extended=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2rshmV2E2Tt",
        "outputId": "e04cd9e0-4973-470b-aed2-cffe141bce82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|       category_code|\n",
            "+--------------------+\n",
            "|    computers.ebooks|\n",
            "|apparel.shoes.sli...|\n",
            "|computers.periphe...|\n",
            "|electronics.video...|\n",
            "|appliances.kitche...|\n",
            "|     sport.snowboard|\n",
            "|electronics.camer...|\n",
            "|       apparel.shirt|\n",
            "|electronics.audio...|\n",
            "|appliances.kitche...|\n",
            "|appliances.kitche...|\n",
            "|appliances.kitche...|\n",
            "|  electronics.tablet|\n",
            "|auto.accessories....|\n",
            "|apparel.shoes.moc...|\n",
            "|       apparel.jeans|\n",
            "|computers.periphe...|\n",
            "|furniture.living_...|\n",
            "| stationery.cartrige|\n",
            "|furniture.kitchen...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select distinct category_code from ecommerce\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnIvyNMJE2Tt"
      },
      "source": [
        "Windowing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xp91qDFE2Tu"
      },
      "outputs": [],
      "source": [
        "# workaround of time parser with Spark 3.0\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GRJb2I4E2Tu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_timestamp, window\n",
        "\n",
        "ts = to_timestamp(df.event_time, \"yyyy-MM-dd HH:mm:ss\")\n",
        "df_ts = df.withColumn(\"time\", ts)\n",
        "grouped_counts = df_ts \\\n",
        "    .groupBy(window(df_ts.time, \"1 hour\")) \\\n",
        "    .count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_oYE-vUE2Tu",
        "outputId": "2a768d73-bd8b-4afa-deab-c6426e14e632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|              window| count|\n",
            "+--------------------+------+\n",
            "|{2019-10-11 08:00...| 84183|\n",
            "|{2019-10-08 13:00...| 79332|\n",
            "|{2019-10-15 16:00...|111779|\n",
            "|{2019-10-11 06:00...| 83501|\n",
            "|{2019-10-13 15:00...|104344|\n",
            "|{2019-10-11 14:00...| 99806|\n",
            "|{2019-10-13 07:00...| 97515|\n",
            "|{2019-10-11 09:00...| 85433|\n",
            "|{2019-10-13 09:00...|112539|\n",
            "|{2019-10-15 05:00...| 74147|\n",
            "|{2019-10-08 17:00...| 88401|\n",
            "|{2019-10-13 14:00...| 94476|\n",
            "|{2019-10-15 14:00...|102932|\n",
            "|{2019-10-11 17:00...| 90579|\n",
            "|{2019-10-11 13:00...| 86093|\n",
            "|{2019-10-11 11:00...| 77983|\n",
            "|{2019-10-13 16:00...|108415|\n",
            "|{2019-10-15 11:00...| 83196|\n",
            "|{2019-10-13 08:00...|108000|\n",
            "|{2019-10-08 07:00...| 79633|\n",
            "+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "grouped_counts.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeUrIitSE2Tv",
        "outputId": "d6c7c10e-d568-4168-ffa1-db57ce44e762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py:140: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  FutureWarning\n"
          ]
        }
      ],
      "source": [
        "dfNov.registerTempTable(\"ecommerce_Nov\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOQVqx32E2Tv",
        "outputId": "4362963b-8167-4e10-f912-3bd9cfeb8a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+------------------+------------------+\n",
            "|  user_id|total_spending_Oct|total_spending_Nov|             total|\n",
            "+---------+------------------+------------------+------------------+\n",
            "|512365995|        1499595.88|         2263807.2|        3763403.08|\n",
            "|512845454|        1116256.84|        1537491.58|        2653748.42|\n",
            "|563459593|1993636.4100000006| 657277.2100000001|2650913.6200000006|\n",
            "|536399452|        1692370.92| 369216.9399999999|2061587.8599999999|\n",
            "|513558661|399091.07999999996|        1557656.67|        1956747.75|\n",
            "+---------+------------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "1640.866041848\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "    with user_spending_Oct as (\n",
        "        select user_id, sum(price) as total_spending_Oct\n",
        "        from ecommerce group by user_id),\n",
        "\n",
        "        user_spending_Nov as (\n",
        "        select user_id, sum(price) as total_spending_Nov \n",
        "        from ecommerce_Nov group by user_id)\n",
        "    \n",
        "    select\n",
        "      *, total_spending_Oct + total_spending_Nov as total\n",
        "    from\n",
        "      user_spending_Oct\n",
        "      join user_spending_Nov using (user_id)\n",
        "      order by total_spending_Oct + total_spending_Nov desc\n",
        "\"\"\").show(5)\n",
        "t1 = timeit.default_timer()\n",
        "print(t1-t0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"\"\"\n",
        "    select user_id, avg(price) as avg_price \n",
        "        from ecommerce\n",
        "        join ecommerce_Nov using(user_id)\n",
        "        group by user_id\n",
        "\"\"\"\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQsVXa1dPZB4",
        "outputId": "0713c8b1-4009-4fb2-b523-9d6abf2129a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    select user_id, avg(price) as avg_price \n",
            "        from ecommerce\n",
            "        join ecommerce_Nov using(user_id)\n",
            "        group by user_id\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4um4ZwQnE2Tv"
      },
      "outputs": [],
      "source": [
        "#spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VHPTZzCE2Tv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "12_E-commerce Demo DAG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3AMm5RAYE2Tb",
        "0YQVOWBkE2Td",
        "CxSJ0TnmE2Tk",
        "ona8_y0mE2Tl",
        "YWkbwHfqE2Tm",
        "dkTUvORQE2Tm",
        "VTG1uy77E2Tn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}